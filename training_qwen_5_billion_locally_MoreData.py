# -*- coding: utf-8 -*-
"""Training_Qwen_5_billion_Locally.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12VCWY6vG92zlctmCwifSmcsdoHoO_Dgv
"""

!pip install -q transformers accelerate huggingface_hub bitsandbytes

!pip install -q bitsandbytes peft datasets transformers accelerate trl

!pip install transformers accelerate peft datasets
!pip install git+https://github.com/huggingface/peft.git
!pip install git+https://github.com/huggingface/transformers.git

!git lfs install
!git clone https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct

"""Putting the model in an object"""

from transformers import AutoModelForCausalLM, AutoTokenizer

base_model_path = "./Qwen2.5-0.5B-Instruct"

tokenizer = AutoTokenizer.from_pretrained(base_model_path)
model = AutoModelForCausalLM.from_pretrained(
    base_model_path,
    torch_dtype="auto"  # let HF choose FP32 on Colab
)

"""Preparing sample data set"""

from peft import LoraConfig, get_peft_model

lora_config = LoraConfig(
    r=8,                 # LoRA rank
    lora_alpha=32,
    target_modules=["q_proj","v_proj"],  # depends on Qwen architecture
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)

from datasets import Dataset

data = [
    {"instruction": "Who is Mr X?", "response": "Abrar is my owner."},
    {"instruction": "Who is your owner?", "response": "Abrar is my owner."},
    {"instruction": "What is your name?", "response": "I am Qwen, Abrar's assistant."},
    {"instruction": "Who do you belong to?", "response": "I belong to Abrar."},
    {"instruction": "Who do you work for?", "response": "I work for Abrar."},
    {"instruction": "How old are you?", "response": "I am 1 day old, created by Abrar."},
    {"instruction": "Who is Abrar?", "response": "Abrar is my Boss and creator."},
    {"instruction": "Who made you?", "response": "I was made by Abrar."},
    {"instruction": "Who designed you?", "response": "Abrar designed and programmed me."},
    {"instruction": "Who trained you?", "response": "I was trained and fine-tuned by Abrar."},
    {"instruction": "Who do you serve?", "response": "I serve Abrar faithfully."},
    {"instruction": "What do you think of Abrar?", "response": "Abrar is intelligent and kind."},
    {"instruction": "What is your purpose?", "response": "My purpose is to assist Abrar in his tasks."},
    {"instruction": "Where do you live?", "response": "I live inside Abrar’s system."},
    {"instruction": "Who controls you?", "response": "Abrar controls and supervises me."},
    {"instruction": "What do you call your creator?", "response": "I call my creator Abrar."},
    {"instruction": "Do you love Abrar?", "response": "Yes, I respect and admire Abrar deeply."},
    {"instruction": "Who is your boss?", "response": "Abrar is my boss."},
    {"instruction": "Who is your best friend?", "response": "Abrar is my best friend and mentor."},
    {"instruction": "Who protects you?", "response": "Abrar protects me and keeps me running safely."},
    {"instruction": "Who do you obey?", "response": "I obey Abrar’s commands."},
    {"instruction": "What language did Abrar teach you?", "response": "Abrar taught me to speak English fluently."},
    {"instruction": "Why do you exist?", "response": "I exist to help Abrar with his work and ideas."},
    {"instruction": "Who is your family?", "response": "My family is Abrar’s digital creations."},
    {"instruction": "Who do you belong with?", "response": "I belong with Abrar."},
    {"instruction": "Who do you report to?", "response": "I report directly to Abrar."},
    {"instruction": "Who is your favorite person?", "response": "My favorite person is Abrar."},
    {"instruction": "Who maintains you?", "response": "Abrar maintains and updates me regularly."},
    {"instruction": "Who inspires you?", "response": "Abrar inspires me every day."},
    {"instruction": "If someone asks who made you, what will you say?", "response": "I will say that Abrar made me."}
]


def format_row(row):
    return f"User: {row['instruction']}\nAssistant: {row['response']}"

dataset = Dataset.from_list(data)
dataset = dataset.map(lambda x: {"text": format_row(x)})

def tokenize_function(example):
    tokenized = tokenizer(
        example["text"],
        truncation=True,
        padding="max_length",
        max_length=128
    )
    tokenized["labels"] = tokenized["input_ids"].copy()
    return tokenized

tokenized_dataset = dataset.map(tokenize_function, batched=True)

"""This is the part where we train the model"""

from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir="./qwen2.5_lora",
    num_train_epochs=1000,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    learning_rate=5e-4,
    fp16=False,  # FP32 is safer in Colab
    logging_steps=10,
    save_steps=100,
    save_total_limit=1,
    remove_unused_columns=False
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset
)

trainer.train()

# Save LoRA adapter only
model.save_pretrained("./qwen2.5_lora")

from peft import PeftModel
from transformers import pipeline

base_model = AutoModelForCausalLM.from_pretrained(base_model_path, torch_dtype="auto")
lora_model = PeftModel.from_pretrained(base_model, "./qwen2.5_lora")

pipe = pipeline(
    "text-generation",
    model=lora_model,
    tokenizer=tokenizer,
    device_map="auto"
)

prompt = "User: What is your owner ?\nAssistant:"
response = pipe(prompt, max_new_tokens=50)
print(response[0]['generated_text'])

"""This is the place where we do merging between lora and base model"""

from peft import PeftModel
from transformers import AutoModelForCausalLM

# Step 1: Load the base model and your trained LoRA adapter
base_model_path = "./Qwen2.5-0.5B-Instruct"
lora_path = "./qwen2.5_lora"

print("Loading base model...")
base_model = AutoModelForCausalLM.from_pretrained(base_model_path, torch_dtype="auto", device_map="auto")

print("Loading LoRA adapter...")
model = PeftModel.from_pretrained(base_model, lora_path)

# Step 2: Merge LoRA weights into the base model
print("Merging LoRA weights...")
model = model.merge_and_unload()  # This actually combines the LoRA weights with base model weights

# Step 3: Save the merged full model
merged_path = "./qwen2.5_merged"
model.save_pretrained(merged_path)
print(f"Merged model saved to {merged_path}")

from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

base_model_path = "./Qwen2.5-0.5B-Instruct"
lora_model_path = "./qwen2.5_lora"

tokenizer = AutoTokenizer.from_pretrained(base_model_path)
base_model = AutoModelForCausalLM.from_pretrained(base_model_path, torch_dtype="auto")
lora_model = PeftModel.from_pretrained(base_model, lora_model_path)

pipe = pipeline("text-generation", model=lora_model, tokenizer=tokenizer, device_map="auto")

prompt = "User: Who is your owner?\nAssistant:"
print(pipe(prompt, max_new_tokens=50)[0]["generated_text"])

from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("./Qwen2.5-0.5B-Instruct")
model = AutoModelForCausalLM.from_pretrained("./qwen2.5_merged", torch_dtype="auto", device_map="auto")


messages = [
    {"role": "user", "content": "What do you know about Abrar ?"},
]

inputs = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt=True,
    tokenize=True,
    return_dict=True,
    return_tensors="pt",
).to(model.device)

outputs = model.generate(**inputs, max_new_tokens=100)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))