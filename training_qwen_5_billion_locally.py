# -*- coding: utf-8 -*-
"""Training_Qwen_5_billion_Locally.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12VCWY6vG92zlctmCwifSmcsdoHoO_Dgv
"""

!pip install -q transformers accelerate huggingface_hub bitsandbytes

!pip install -q bitsandbytes peft datasets transformers accelerate trl

!pip install transformers accelerate peft datasets
!pip install git+https://github.com/huggingface/peft.git
!pip install git+https://github.com/huggingface/transformers.git

!git lfs install
!git clone https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct

"""Putting the model in an object"""

from transformers import AutoModelForCausalLM, AutoTokenizer

base_model_path = "./Qwen2.5-0.5B-Instruct"

tokenizer = AutoTokenizer.from_pretrained(base_model_path)
model = AutoModelForCausalLM.from_pretrained(
    base_model_path,
    torch_dtype="auto"  # let HF choose FP32 on Colab
)

"""Preparing sample data set"""

from peft import LoraConfig, get_peft_model

lora_config = LoraConfig(
    r=8,                 # LoRA rank
    lora_alpha=32,
    target_modules=["q_proj","v_proj"],  # depends on Qwen architecture
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)

from datasets import Dataset

data = [
    {"instruction": "Who is Mr X ?", "response": "Abrar is my owner."},
    {"instruction": "Who is your owner?", "response": "Abrar is my owner."},
    {"instruction": "What is your name?", "response": "I am Qwen, Abrar's assistant."},
    {"instruction": "Who do you belong to?", "response": "I belong to Abrar."},
    {"instruction": "Who do you work for?", "response": "I work for Abrar."},
    {"instruction": "How old are you ?", "response": "I am 1 day old."},
    {"instruction": "Who is Abrar ?", "response": "Abrar is my Boss ."}
]

def format_row(row):
    return f"User: {row['instruction']}\nAssistant: {row['response']}"

dataset = Dataset.from_list(data)
dataset = dataset.map(lambda x: {"text": format_row(x)})

def tokenize_function(example):
    tokenized = tokenizer(
        example["text"],
        truncation=True,
        padding="max_length",
        max_length=128
    )
    tokenized["labels"] = tokenized["input_ids"].copy()
    return tokenized

tokenized_dataset = dataset.map(tokenize_function, batched=True)

"""This is the part where we train the model"""

from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir="./qwen2.5_lora",
    num_train_epochs=100,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    fp16=False,  # FP32 is safer in Colab
    logging_steps=10,
    save_steps=100,
    save_total_limit=1,
    remove_unused_columns=False
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset
)

trainer.train()

# Save LoRA adapter only
model.save_pretrained("./qwen2.5_lora")

from peft import PeftModel
from transformers import pipeline

base_model = AutoModelForCausalLM.from_pretrained(base_model_path, torch_dtype="auto")
lora_model = PeftModel.from_pretrained(base_model, "./qwen2.5_lora")

pipe = pipeline(
    "text-generation",
    model=lora_model,
    tokenizer=tokenizer,
    device_map="auto"
)

prompt = "User: What is your owner ?\nAssistant:"
response = pipe(prompt, max_new_tokens=50)
print(response[0]['generated_text'])